# -*- coding: utf-8 -*-
"""blueAI_langchain_rag_chroma_gpt_toJson_v7

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-l9P4RoulN77mYL4VxHUWd5D0YviKjqj

## Assistant + tools
- **code_interpreter**
   - 토큰 길이에 상관 없이, 코드를 생성해서 답변할 수 있는 도구 입니다.
   - 예: 피보나치 수열을 만들어줘 -> 피보나치 수열을 생성하는 함수를 생성한 후 이것의 실행 결과로 답변을 만듦 (cf., Code Interpreter가 아니라면, text 기반의 sequence generation으로 답변을 함)
   - 함수를 생성하는 방식이기 때문에, input, output의 token length 제약을 벗어나서 활용을 할 수 있습니다.
   - 데이터와 그래프 이미지가 포함된 파일을 생성할 수 있습니다.
- **[이번 내용] file_search**
   - 우리가 업로드한 자료에서 검색한 이후 답변을 만들어 줍니다. (RAG를 활용하는 것과 비슷)
   - Foundation model을 sFT하는데 걸리는 시간을 생각하면, 대부분의 짧은 배치는 RAG 형태로 구성하고, 긴배치(ex., 1~2주일에 한번)는 sFT를 하는 방식이 적절할 것으로 보입니다.
   - File을 Vector Store에 넣어두고 Assistant가 이를 활용할 수 있습니다.
   - 파일 하나당 최대 크기 512MB, 최대 토큰수 5,000,000까지 지원합니다.
   - [지원하는 파일 타입](https://platform.openai.com/docs/assistants/tools/file-search/supported-files)
   - [Known Limitation](https://platform.openai.com/docs/assistants/tools/file-search/how-it-works)
   - 아직 이미지 파일이나, csv, jsonl을 지원하지 않는데 추후 업데이트 될 것으로 보입니다. (24-05-21 기준)
- **function**
   - 우리가 직접 만든 함수를 사용할 수 있게 합니다.

\

## 기본 작업
- Open AI 라이브러리 설치
- Dotenv 설치
- Google Drive 연결
- API Key 로드
- Open AI 클라이언트 객체 생성
"""

from dotenv import load_dotenv
import json
from openai import OpenAI
import os
from langchain_community.embeddings import SentenceTransformerEmbeddings
from langchain.document_loaders import PyPDFDirectoryLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.schema.document import Document
from langchain.vectorstores import Chroma
from langchain.chains.question_answering import load_qa_chain
from langchain.chains import LLMChain, StuffDocumentsChain
from langchain.chat_models import ChatOpenAI  # ChatOpenAI를 사용할 경우
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain_core.prompts import ChatPromptTemplate
import xml.etree.ElementTree as ET
from datetime import datetime, timezone, timedelta

# 베이스 디렉토리 설정
BASE_DIR = "bebot_templates\\"
DATA_PATH = "ragData"

api_key = os.environ.get("OPEN_API_KEY")
client = OpenAI(api_key=api_key)

# JSON 파일 읽기
def read_json_file(filename):
    file_path = BASE_DIR + filename
    with open(file_path, 'r', encoding='utf-8') as file:
        data = json.load(file)
    return data

# XML 파일 읽기
def read_xml_file(filename):
    file_path = BASE_DIR + filename
    tree = ET.parse(file_path)
    root = tree.getroot()
    return root

# TXT 파일 읽기
def read_txt_file(filename):
    file_path = BASE_DIR + filename
    with open(file_path, 'r', encoding='utf-8') as file:
        data = file.read()
    return data
  
"""### json템플릿과 prompt"""
xaml_template = read_txt_file("xaml_template.txt")
json_template = read_json_file("json_template.json")
prompt = read_txt_file("prompt.txt")

load_dotenv(
    dotenv_path='.env',
    verbose=True,
)



"""## File-Search Assistant 생성

## Upload files and add them to a Vector Store
- Vector store를 하나 만들고 파일을 업로드 한 후 모든 파일의 업로드 상태가 종료되었는지 확인해야 합니다.
"""

# 폴더 전체로 넣을 경우

document_loader = PyPDFDirectoryLoader(DATA_PATH)
documents = document_loader.load()

# 문서를 청크 단위로 자름
def split_documents(documents: list[Document]):
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=800,
        chunk_overlap=80,
        length_function=len,
        is_separator_regex=False,
    )
    return text_splitter.split_documents(documents)

chunks = split_documents(documents)
# chunks[0]

"""vector db로 임베딩"""

# 임베딩 모델
embeddings = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")

# 벡터 데이터베이스에 적재
vector_database = Chroma.from_documents(documents = chunks, embedding = embeddings)

# Define the RAG setup
retriever = vector_database.as_retriever()
# question  ="네이버에서 사과를 검색해줘"

# llm모델 가져오기
llm = ChatOpenAI(api_key=api_key, model_name="gpt-4o", temperature=0)

# rag chain구성 틀
chain = load_qa_chain(llm, chain_type="stuff",verbose=True)

# rag chain에 넣을 prompt구성
QA_CHAIN_PROMPT = PromptTemplate.from_template(prompt)

xmlresult = ""


def gpt_llm(query):
  # Retrieval QA
  qa_chain = RetrievalQA.from_chain_type(
      llm=llm,
      retriever=vector_database.as_retriever(),
      chain_type_kwargs={"prompt": QA_CHAIN_PROMPT}
  )

  # 가장 관련성 높은 문서 1개 검색
  retrieved_docs = vector_database.similarity_search(query, k=7)  

  # 검색된 문서들에서 context 결합하기
  # 7개의 문서 내용을 결합
  if retrieved_docs:
      context = "\n\n".join([doc.page_content for doc in retrieved_docs])  
      # print("Retrieved Context:", context)
  else:
      print("No relevant context found.")

  # 입력값을 "context"와 "x"로 묶어서 전달
  input_data = {
      "context": context,
      "query": query
  }

  result = qa_chain(input_data)

  # print(result["result"])

  return result["result"]

def generate_response(messages):
    # Create a loading spinner
    spinner = Halo(text='Loading...', spinner='dots')
    spinner.start()

def rag_chain(query):
    retrieved_docs = retriever.invoke(query)
    formatted_context = "\n\n".join(doc.page_content for doc in retrieved_docs)
    # return gpt_llm(question, formatted_context)
    return gpt_llm(query)

################################################
### 실행 라인!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
### 만약 flask를 활용하지 않고, command라인에서 실행하고 싶다면, 아래의 주석을 해제하고 실행해보시면 됩니다.
### cmd 명령어 : `python blueai_langchain_rag_chroma_gpt_tojson_v7.py`
# question = input()
# xmlresult = get_important_facts(question)
#
# LLM이 반환한 결과 확인
# print(xmlresult)
##################################################

"""# 여기까지가 xaml을 출력하는 부 입니다.

v7에서는 해당 xaml을 json형식에 맞게 가공하는 절차가 추가됩니다.
"""

"""# LLM이 제작한 핵심 xaml을 bebot에 들어갈 json형식으로 변환

1. xaml형식 변환 : 엔티티 문자(&quot; 등) 들에 대한 고려가 필요함
2. json템플릿에 xaml주입 : 정제된 xaml을 json에 넣어 최종적으로 bebot에 돌아갈 형식으로 구성

## 1. xaml형식 변환
"""

# 이스케이프 처리를 위한 백슬래시 추가
# html 엔티티추가 문제를 해결하는 함수입니다.
def convert_xml_to_html_entities(xml_str):
    # selector 문자열에서 모든 큰따옴표를 \"로 대체
    selector = xml_str.replace(r'\\', "").replace(r"`xml",'').replace(r"`",'')
    # print("hello")
    start_search_pos = 0
    # print(selector)

    while True:
        # "Selector=" 위치 찾기
        title_start = selector.find("Selector=", start_search_pos)
        print(title_start)
        if title_start == -1:
            break  # 더 이상 "Selector="가 없으면 종료

        # "Selector=" 뒤의 값을 찾기 위해 시작 위치를 조정
        title_start += 10  # "Selector="의 길이만큼 건너뛰기
        title_end = selector.find("]\">", title_start)
        # print(title_end)
        if title_end == -1:
            
          break  # 잘못된 형식이거나 더 이상 닫는 대괄호가 없으면 종료

        # title 속성 값 추출
        title_value = selector[title_start:title_end]
        print(title_value)
        # title 속성 값에서 <와 >를 &lt;와 &gt;, "를 &quot;로 대체
        # title_value = title_value.replace(r"\&quot;", "&quot;").replace("<", "&lt;").replace(">", "&gt;")

        title_value = title_value.replace('\"', "&quot;").replace("<", "&lt;").replace(">", "&gt;")
        # 수정된 title 값을 사용해 selector 문자열 재구성
        print(f"바뀐값 : {title_value}")

        selector = selector[:title_start] + title_value + selector[title_end:]

        # 다음 "Selector="를 찾기 위해 검색 시작 위치 조정
        start_search_pos = title_end

    return selector


"""## 2. json 템플릿에 추가"""

# JSON 데이터로 변환
# 예시
# json_data = {
#     "queue": none,
#     "Xml": converted_xml_data
# }


# 현재 시간을 가져와서 ISO 8601 형식으로 변환
def get_current_time():
    # 현재 시간에 타임존 정보 추가 (예: UTC+9:00)
    currented_time = datetime.now(timezone(timedelta(hours=9)))
    # ISO 8601 형식으로 변환
    return currented_time.strftime("%Y-%m-%dT%H:%M:%S.%f%z")
  
  

# 저장한 JSON 파일을 읽어오는 함수
def read_json(file_path):
    with open(file_path, 'r', encoding='utf-8') as json_file:
        data = json.load(json_file)
    return data

# JSON 파일을 다시 저장하는 함수
def save_json(file_path, data):
    with open(file_path, 'w', encoding='utf-8') as json_file:
        json.dump(data, json_file, ensure_ascii=False, indent=2)
    print(f"JSON 파일이 {file_path}에 저장되었습니다.")

# 템플릿에 값 채우기
def set_json_template(question, final_output_xaml):
  json_template["Xaml"] = final_output_xaml
  json_template["Filename"] = json_template["Filename"].format(question=question)
  json_template["projectandname"] = json_template["projectandname"].format(question=question)
  json_template["FilePath"] = json_template["FilePath"].format(question=question)
  json_template["name"] = json_template["name"].format(question=question)
  json_template["_created"] = get_current_time()

  # 결과 확인
  final_json_str = json.dumps(json_template, ensure_ascii=False, indent=2)
  # print(final_json_str)

  # 필요하다면 파일로 저장
  output_directory = "/content/blueAI_createdJson"
  os.makedirs(output_directory, exist_ok=True)
  file_path = os.path.join(output_directory, f"{question}.json")

  with open(file_path, 'w', encoding='utf-8') as json_file:
      json_file.write(final_json_str)
  print(f"JSON 파일이 {file_path}에 저장되었습니다.")

# XML 데이터를 HTML 엔티티로 변환 후 출력
# converted_xml_data = convert_xml_to_html_entities(xmlresult)
# print(converted_xml_data)



  # 파일 경로 설정 (여기서는 예시로 '/content/blueAI_createdJson/네이버에서 사과를 검색해줘.json'로 가정)
  # 파일 경로 설정 (D 드라이브의 content 폴더 내에 위치)
  output_directory = "D:/content/blueAI_createdJson/"+question+".json"
  # file_path = os.path.join(output_directory, question)
  file_path = "D:/content/blueAI_createdJson/"+question+".json"

  # 1. 저장한 파일을 읽어옴
  json_data = read_json(file_path)

  # 2. Xaml의 값을 가져옴
  xmlVal = json_data.get("Xaml", "")

  # 3. XML 데이터를 HTML 엔티티로 변환
  converted_xml_data = convert_xml_to_html_entities(xmlVal)

  # 4. 변환된 데이터를 다시 Xaml에 저장
  json_data["Xaml"] = converted_xml_data

  # 5. 변경된 데이터를 다시 JSON 파일로 저장
  save_json(file_path, json_data)


# Define the Gradio interface
def get_important_facts(query):

    xmlresult = rag_chain(query)
    # print("*********")
    # print(xmlresult)
    final_output_xaml = xaml_template.format(custom_code=xmlresult)
    set_json_template(query, final_output_xaml)
    return xmlresult